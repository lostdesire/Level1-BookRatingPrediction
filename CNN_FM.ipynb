{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils import Logger, Setting\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam\n",
    "import random\n",
    "import easydict\n",
    "import re\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'data_path': './data/',  # Data path 설정\n",
    "    'saved_model_path': './saved_models',  # Saved Model path 설정\n",
    "    'model': 'CNN_FM',  # 학습 및 예측할 모델 선택 (None으로 초기화, 사용 전에 설정 필요)\n",
    "    'data_shuffle': True,  # 데이터 셔플 여부 조정\n",
    "    'test_size': 0.2,  # Train/Valid split 비율 조정\n",
    "    'seed': 42,  # Seed 값 조정\n",
    "    'use_best_model': True,  # 검증 성능이 가장 좋은 모델 사용 여부 설정\n",
    "\n",
    "    # TRAINING OPTION\n",
    "    'batch_size': 1024,  # Batch size 조정\n",
    "    'epochs': 10,  # Epoch 수 조정\n",
    "    'lr': 1e-3,  # Learning Rate 조정\n",
    "    'loss_fn': 'RMSE',  # 손실 함수 변경 (MSE 또는 RMSE)\n",
    "    'optimizer': 'ADAM',  # 최적화 함수 변경 (SGD 또는 ADAM)\n",
    "    'weight_decay': 1e-6,  # Adam optimizer에서 정규화에 사용하는 값 조정\n",
    "\n",
    "    # GPU\n",
    "    'device': 'cuda',  # 학습에 사용할 Device 조정\n",
    "\n",
    "    # FM, FFM, NCF, WDN, DCN Common OPTION\n",
    "    'embed_dim': 16,  # FM, FFM, NCF, WDN, DCN에서 embedding시킬 차원 조정\n",
    "    'dropout': 0.2,  # NCF, WDN, DCN에서 Dropout rate 조정\n",
    "    'mlp_dims': (16, 16),  # NCF, WDN, DCN에서 MLP Network의 차원 조정\n",
    "\n",
    "    # CNN_FM\n",
    "    'cnn_embed_dim': 64,  # CNN_FM에서 user와 item에 대한 embedding시킬 차원 조정\n",
    "    'cnn_latent_dim': 12,  # CNN_FM에서 user/item/image에 대한 latent 차원 조정\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Setting:\n",
    "    @staticmethod\n",
    "    def seed_everything(seed):\n",
    "        \n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    def __init__(self):\n",
    "        now = time.localtime()\n",
    "        now_date = time.strftime('%Y%m%d', now)\n",
    "        now_hour = time.strftime('%X', now)\n",
    "        save_time = now_date + '_' + now_hour.replace(':', '')\n",
    "        self.save_time = save_time\n",
    "\n",
    "    def get_log_path(self, args):\n",
    "        \n",
    "        path = f'./log/{self.save_time}_{args.model}/'\n",
    "        return path\n",
    "\n",
    "    def get_submit_filename(self, args):\n",
    "        \n",
    "        filename = f'./submit/{self.save_time}_{args.model}.csv'\n",
    "        return filename\n",
    "\n",
    "    def make_dir(self,path):\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        else:\n",
    "            pass\n",
    "        return path\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, args, path):\n",
    "        \n",
    "        self.args = args\n",
    "        self.path = path\n",
    "\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.formatter = logging.Formatter('[%(asctime)s] - %(message)s')\n",
    "\n",
    "        self.file_handler = logging.FileHandler(self.path+'train.log')\n",
    "        self.file_handler.setFormatter(self.formatter)\n",
    "        self.logger.addHandler(self.file_handler)\n",
    "\n",
    "    def log(self, epoch, train_loss, valid_loss):\n",
    "        \n",
    "        message = f'epoch : {epoch}/{self.args.epochs} | train loss : {train_loss:.3f} | valid loss : {valid_loss:.3f}'\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def close(self):\n",
    "       \n",
    "        self.logger.removeHandler(self.file_handler)\n",
    "        self.file_handler.close()\n",
    "\n",
    "    def save_args(self):\n",
    "        \n",
    "        argparse_dict = self.args.__dict__\n",
    "\n",
    "        with open(f'{self.path}/model.json', 'w') as f:\n",
    "            json.dump(argparse_dict,f,indent=4)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = Setting()\n",
    "log_path = setting.get_log_path(args)\n",
    "setting.make_dir(log_path)\n",
    "\n",
    "logger = Logger(args, log_path)\n",
    "logger.save_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_vector(path):\n",
    "    \n",
    "    img = Image.open(path)\n",
    "    scale = transforms.Resize((32, 32))\n",
    "    tensor = transforms.ToTensor()\n",
    "    img_fe = Variable(tensor(scale(img)))\n",
    "    return img_fe\n",
    "\n",
    "def process_img_data(df, books, user2idx, isbn2idx, train=False):\n",
    "\n",
    "    books_ = books.copy()\n",
    "    books_['isbn'] = books_['isbn'].map(isbn2idx)\n",
    "\n",
    "    if train == True:\n",
    "        df_ = df.copy()\n",
    "    else:\n",
    "        df_ = df.copy()\n",
    "        df_['user_id'] = df_['user_id'].map(user2idx)\n",
    "        df_['isbn'] = df_['isbn'].map(isbn2idx)\n",
    "\n",
    "    df_ = pd.merge(df_, books_[['isbn', 'img_path']], on='isbn', how='left')\n",
    "    df_['img_path'] = df_['img_path'].apply(lambda x: 'data/'+x)\n",
    "    img_vector_df = df_[['img_path']].drop_duplicates().reset_index(drop=True).copy()\n",
    "    data_box = []\n",
    "    for idx, path in tqdm(enumerate(sorted(img_vector_df['img_path']))):\n",
    "        data = image_vector(path)\n",
    "        if data.size()[0] == 3:\n",
    "            data_box.append(np.array(data))\n",
    "        else:\n",
    "            data_box.append(np.array(data.expand(3, data.size()[1], data.size()[2])))\n",
    "    img_vector_df['img_vector'] = data_box\n",
    "    df_ = pd.merge(df_, img_vector_df, on='img_path', how='left')\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129777it [00:59, 2198.14it/s]\n",
      "52000it [00:23, 2193.10it/s]\n"
     ]
    }
   ],
   "source": [
    "path = './data/'\n",
    "\n",
    "users = pd.read_csv(path+'users.csv')\n",
    "books = pd.read_csv(path+'books.csv')\n",
    "train = pd.read_csv(path + 'train_ratings.csv')\n",
    "test = pd.read_csv(path + 'test_ratings.csv')\n",
    "sub = pd.read_csv(path + 'sample_submission.csv')\n",
    "\n",
    "ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "\n",
    "idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "train['user_id'] = train['user_id'].map(user2idx)\n",
    "sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "\n",
    "train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "sub['isbn'] = sub['isbn'].map(isbn2idx)\n",
    "\n",
    "img_train = process_img_data(train, books, user2idx, isbn2idx, train=True)\n",
    "img_test = process_img_data(test, books, user2idx, isbn2idx, train=False)\n",
    "\n",
    "data = {\n",
    "        'train':train,\n",
    "        'test':test,\n",
    "        'users':users,\n",
    "        'books':books,\n",
    "        'sub':sub,\n",
    "        'idx2user':idx2user,\n",
    "        'idx2isbn':idx2isbn,\n",
    "        'user2idx':user2idx,\n",
    "        'isbn2idx':isbn2idx,\n",
    "        'img_train':img_train,\n",
    "        'img_test':img_test,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                        data['img_train'][['user_id', 'isbn', 'img_vector']],\n",
    "                                                        data['img_train']['rating'],\n",
    "                                                        test_size=args.test_size,\n",
    "                                                        random_state=args.seed,\n",
    "                                                        shuffle=True\n",
    "                                                       )\n",
    "data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset(Dataset):\n",
    "    def __init__(self, user_isbn_vector, img_vector, label):\n",
    "        self.user_isbn_vector = user_isbn_vector\n",
    "        self.img_vector = img_vector\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return self.user_isbn_vector.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "                'user_isbn_vector' : torch.tensor(self.user_isbn_vector[i], dtype=torch.long),\n",
    "                'img_vector' : torch.tensor(self.img_vector[i], dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label[i], dtype=torch.float32),\n",
    "                }\n",
    "\n",
    "train_dataset = Image_Dataset(\n",
    "                                data['X_train'][['user_id', 'isbn']].values,\n",
    "                                data['X_train']['img_vector'].values,\n",
    "                                data['y_train'].values\n",
    "                            )\n",
    "valid_dataset = Image_Dataset(\n",
    "                                data['X_valid'][['user_id', 'isbn']].values,\n",
    "                                data['X_valid']['img_vector'].values,\n",
    "                                data['y_valid'].values\n",
    "                            )\n",
    "test_dataset = Image_Dataset(\n",
    "                                data['img_test'][['user_id', 'isbn']].values,\n",
    "                                data['img_test']['img_vector'].values,\n",
    "                                data['img_test']['rating'].values\n",
    "                            )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, num_workers=0, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.batch_size, num_workers=0, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, num_workers=0, shuffle=False)\n",
    "data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN_FM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Parameter(torch.rand(input_dim, latent_dim), requires_grad = True)\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        square_of_sum = torch.mm(x, self.v) ** 2\n",
    "        sum_of_square = torch.mm(x ** 2, self.v ** 2)\n",
    "        pair_interactions = torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)\n",
    "        output = linear + (0.5 * pair_interactions)\n",
    "        return output\n",
    "\n",
    "\n",
    "# factorization을 통해 얻은 feature를 embedding 합니다.\n",
    "class FeaturesEmbedding(nn.Module):\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int32)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# 이미지 특징 추출을 위한 기초적인 CNN Layer를 정의합니다.\n",
    "class CNN_Base(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(CNN_Base, self).__init__()\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "                                        nn.Conv2d(3, 6, kernel_size=3, stride=2, padding=1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                        nn.Conv2d(6, 12, kernel_size=3, stride=2, padding=1),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layer(x)\n",
    "        x = x.view(-1, 12 * 1 * 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 기존 유저/상품 벡터와 이미지 벡터를 결합하여 FM으로 학습하는 모델을 구현합니다.\n",
    "class CNN_FM(torch.nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super().__init__()\n",
    "        self.field_dims = np.array([len(data['user2idx']), len(data['isbn2idx'])], dtype=np.uint32)\n",
    "        self.embedding = FeaturesEmbedding(self.field_dims, args.cnn_embed_dim)\n",
    "        self.cnn = CNN_Base()\n",
    "        self.fm = FactorizationMachine(\n",
    "                                        input_dim=(args.cnn_embed_dim * 2) + (12 * 1 * 1),\n",
    "                                        latent_dim=args.cnn_latent_dim,\n",
    "                                        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_isbn_vector, img_vector = x[0], x[1]\n",
    "        user_isbn_feature = self.embedding(user_isbn_vector)\n",
    "        img_feature = self.cnn(img_vector)\n",
    "        feature_vector = torch.cat([\n",
    "                                    user_isbn_feature.view(-1, user_isbn_feature.size(1) * user_isbn_feature.size(2)),\n",
    "                                    img_feature\n",
    "                                    ], dim=1)\n",
    "        output = self.fm(feature_vector)\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.eps = 1e-6\n",
    "    def forward(self, x, y):\n",
    "        criterion = MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y)+self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train(args, model, dataloader, logger, setting):\n",
    "    minimum_loss = 999999999\n",
    "    if args.loss_fn == 'MSE':\n",
    "        loss_fn = MSELoss()\n",
    "    elif args.loss_fn == 'RMSE':\n",
    "        loss_fn = RMSELoss()\n",
    "    else:\n",
    "        pass\n",
    "    if args.optimizer == 'SGD':\n",
    "        optimizer = SGD(model.parameters(), lr=args.lr)\n",
    "    elif args.optimizer == 'ADAM':\n",
    "        optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(args.epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch = 0\n",
    "\n",
    "        for idx, data in enumerate(dataloader['train_dataloader']):\n",
    "            \n",
    "            x, y = [data['user_isbn_vector'].to(args.device), data['img_vector'].to(args.device)], data['label'].to(args.device)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y.float(), y_hat)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch +=1\n",
    "\n",
    "            # Log train loss\n",
    "            # wandb.log({\"Train Loss\": total_loss/batch})\n",
    "\n",
    "        valid_loss = valid(args, model, dataloader, loss_fn)\n",
    "\n",
    "        # Log valid loss\n",
    "        # wandb.log({\"Valid Loss\": valid_loss})\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Train_loss: {total_loss/batch:.3f}, valid_loss: {valid_loss:.3f}')\n",
    "        logger.log(epoch=epoch+1, train_loss=total_loss/batch, valid_loss=valid_loss)\n",
    "        if minimum_loss > valid_loss:\n",
    "            minimum_loss = valid_loss\n",
    "            os.makedirs(args.saved_model_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'{args.saved_model_path}/{setting.save_time}_{args.model}_model.pt')\n",
    "    logger.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def valid(args, model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batch = 0\n",
    "\n",
    "    for idx, data in enumerate(dataloader['valid_dataloader']):\n",
    "        \n",
    "        x, y = [data['user_isbn_vector'].to(args.device), data['img_vector'].to(args.device)], data['label'].to(args.device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y.float(), y_hat)\n",
    "        total_loss += loss.item()\n",
    "        batch +=1\n",
    "    valid_loss = total_loss/batch\n",
    "    return valid_loss\n",
    "\n",
    "\n",
    "def test(args, model, dataloader, setting):\n",
    "    predicts = list()\n",
    "    if args.use_best_model == True:\n",
    "        model.load_state_dict(torch.load(f'./saved_models/{setting.save_time}_{args.model}_model.pt'))\n",
    "    else:\n",
    "        pass\n",
    "    model.eval()\n",
    "\n",
    "    for idx, data in enumerate(dataloader['test_dataloader']):\n",
    "        \n",
    "        x, _ = [data['user_isbn_vector'].to(args.device), data['img_vector'].to(args.device)], data['label'].to(args.device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        predicts.extend(y_hat.tolist())\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_FM(args, data).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:13<02:00, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_loss: 2.873, valid_loss: 2.476\n",
      "Epoch: 2, Train_loss: 1.919, valid_loss: 2.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:38<01:28, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_loss: 1.660, valid_loss: 2.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:50<01:14, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_loss: 1.571, valid_loss: 2.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:02<01:02, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_loss: 1.519, valid_loss: 2.363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:15<00:49, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_loss: 1.478, valid_loss: 2.370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:27<00:37, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train_loss: 1.447, valid_loss: 2.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:39<00:24, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train_loss: 1.423, valid_loss: 2.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:52<00:12, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train_loss: 1.402, valid_loss: 2.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:04<00:00, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train_loss: 1.381, valid_loss: 2.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(args, model, data, logger, setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = test(args, model, data, setting)\n",
    "\n",
    "submission = pd.read_csv(args.data_path + 'sample_submission.csv')\n",
    "submission['rating'] = predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11676</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>7.126083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116866</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8.419352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152827</td>\n",
       "      <td>0060973129</td>\n",
       "      <td>8.262862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157969</td>\n",
       "      <td>0374157065</td>\n",
       "      <td>8.165571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67958</td>\n",
       "      <td>0399135782</td>\n",
       "      <td>8.503297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>278543</td>\n",
       "      <td>1576734218</td>\n",
       "      <td>7.840802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76695</th>\n",
       "      <td>278563</td>\n",
       "      <td>3492223710</td>\n",
       "      <td>8.574254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76696</th>\n",
       "      <td>278633</td>\n",
       "      <td>1896095186</td>\n",
       "      <td>6.365067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>278668</td>\n",
       "      <td>8408044079</td>\n",
       "      <td>5.097304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76698</th>\n",
       "      <td>278851</td>\n",
       "      <td>0767907566</td>\n",
       "      <td>6.043515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76699 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id        isbn    rating\n",
       "0        11676  0002005018  7.126083\n",
       "1       116866  0002005018  8.419352\n",
       "2       152827  0060973129  8.262862\n",
       "3       157969  0374157065  8.165571\n",
       "4        67958  0399135782  8.503297\n",
       "...        ...         ...       ...\n",
       "76694   278543  1576734218  7.840802\n",
       "76695   278563  3492223710  8.574254\n",
       "76696   278633  1896095186  6.365067\n",
       "76697   278668  8408044079  5.097304\n",
       "76698   278851  0767907566  6.043515\n",
       "\n",
       "[76699 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
